
\documentclass[11pt,a4paper,computermodern]{article}


\usepackage[
%	includeheadfoot,
	width=172mm,
	top=18mm,
	bottom=18mm,
	bindingoffset=4mm
	]{geometry}


%%% Typeface packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fontawesome}

\usepackage{multirow}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage[flushleft]{threeparttable}
\usepackage{enumitem}

%%% Graphics packages
\usepackage{graphicx}

%%
\usepackage{hyperref}
\hypersetup{
	colorlinks=false,
	hidelinks=true,
}


%%
\newcommand{\code}{\texttt}

%%
\title{Stripe Data Architecture}
\date{}


\begin{document}

\maketitle

\vspace{-10mm}

Stripe is a leading global financial technology company, founded in 2010, that powers online payment processing for millions of businesses across over 120 countries. With billions of transactions processed annually and clients ranging from startups to Fortune 500 companies, Stripe operates at massive scale and complexity. As its operations have grown, Stripe's data architecture has become a strategic priority, requiring the integration in a single system of a large variety of data. This proposal outlines a comprehensive data infrastructure designed to ensure performance, consistency, and compliance while enabling advanced use cases such as fraud detection, customer insights, and predictive analytics.


\section*{Architecture Overview}

The data integration architecture is presented in figure~\ref{fig:architecture}. It follows a hybrid model combining real-time streaming and batch processing, supporting low-latency data sync for operational use cases (e.g., fraud detection) and high-throughput batch processing for analytics.
\begin{itemize}[itemsep=5pt, parsep=0pt]
	\item Data streams originating from Stripe API (e.g. bank transaction information, telemetry) are pipelined to the relevant database systems using kafka streams.
	\item A reference database holds slowly changing reference data such as country and currency codes, merchant information of currency exchange rates. Any change in this data is reflected to the systems that depend on it through change data capture (CDC).
	\item Data is archived periodically in a data lake, with batch processing managed by Apache Airflow.
	\item The loading of less time-sensitive data such as audit logs or customer feedback is handled by Airflow batches.
\end{itemize}


\begin{figure}[!htb]
	\centering
	\includegraphics[scale=0.93]{./figures/architecture}
	\caption{Overview of the proposed data architecture. The description is given in the text. The lock symbols in the OLTP and Reference databases indicate that the security of these systems is critical.}
	\label{fig:architecture}
\end{figure}


We present in table~\ref{table:providers} a list of possible providers for the various systems of our architecture.

\begin{table}[!htb]
	\centering
	\begin{threeparttable}
		\caption{Proposed providers for the various systems of the architecture.}
		\label{table:providers}
		\begin{tabularx}{0.6\textwidth}{c >{\centering\arraybackslash}X}
			\toprule
			\multicolumn{1}{c}{\textbf{System}} & \multicolumn{1}{c}{\textbf{Provider}} \\
			\midrule
			OLTP/OLAP & Snowflake, Redshift \\
			NoSQL & MongoDB, DynamoDB \\
			Data Lake & Amazon S3, Azure Data Lake \\
			\bottomrule
		\end{tabularx}
	\end{threeparttable}
\end{table}



\clearpage
\section*{Reference Database}

Slowly changing or static reference data (e.g. country reference, merchant information, currency change rates) is stored in a reference database that serves as a single source of truth. The OLTP, OLAP and NoSQL systems incorporate a copy of the relevant reference tables for faster access. Central updates are propagated through change data capture (CDC). The data is subject to the security and compliance policy described below (e.g. encryption of merchant information). This approach has the advantage of a finer-grained monitoring and control of data access and modification. We present the reference database structure in figure~\ref{fig:Reference} along with the corresponding data dictionary in table~\ref{table:ReferenceDataDict}.

\begin{figure}[!htb]
	\centering
	\includegraphics[scale=0.5]{./figures/Ref}
	\caption{Proposed reference database structure.}
	\label{fig:Reference}
\end{figure}

\begin{table}[!htb]
	\centering
	\begin{threeparttable}
		\caption{Data dictionary for the \code{transactions} OLTP schema.}
		\label{table:ReferenceDataDict}
		\begin{tabularx}{0.99\textwidth}{l c >{\arraybackslash}X >{\arraybackslash}X}
			\toprule
			\multicolumn{1}{c}{\textbf{Field Name}} & \multicolumn{1}{c}{\textbf{Type}} & \multicolumn{1}{c}{\textbf{Description}} & \multicolumn{1}{c}{\textbf{Example}} \\
			\midrule
			\multicolumn{4}{c}{\code{countries} table}\\
			\code{country\_code} & \code{char(2)} & ISO 3166-1 2-letter & \code{'GB'} \\
			\code{country\_name} & \code{text} & Country name & \code{'United Kingdom'} \\
			
			\midrule
			\multicolumn{4}{c}{\code{currencies} table}\\
			\code{currency\_code} & \code{char(3)} & Currency code (ISO 4217) & \code{'EUR'} \\
			\code{currency\_name} & \code{text} & Currency name & \code{'Euro'} \\
			\code{usd\_change\_rate} & \code{char(2)} & Currency to USD change rate & \code{1.08} \\
			
			\midrule
			\multicolumn{4}{c}{\code{merchants} table}\\
			\code{merchant\_id} & \code{bigint} & Unique merchant id & \code{12345} \\
			\code{name} & \code{text} & Merchant name & \code{'Amazon UK'} \\
			\code{iban} & \code{text} & Merchant IBAN & \code{'GB82WEST12345678765432'} \\
			\code{country\_code} & \code{char(2)} & Merchant registration country & \code{'GB'} \\
			
			\midrule
			\multicolumn{4}{c}{\code{customers} table}\\
			\code{customer\_id} & \code{bigint} & Unique customer id & \code{234567} \\
			\code{name} & \code{text} & Customer name & \code{'John Doe'} \\
			\code{iban} & \code{text} & Customer IBAN & \code{'GB82WEST12345678765432'} \\
			\code{country\_code} & \code{char(2)} & Customer country code  & \code{'GB'} \\
			
			\bottomrule
		\end{tabularx}
	\end{threeparttable}
\end{table}


\clearpage
\section*{Online Transaction Processing (OLTP) Data Model}

The OLTP data model is designed to handle core transactional operations with high integrity, performance, and reliability. This includes payment processing, refunds, subscriptions, and fraud indicators. Given the high volume and critical nature of this data, the schema follows normalization principles (3NF) to reduce redundancy and enforce consistency across distributed systems.

Our proposed OLTP architecture is presented in figure~\ref{fig:OLTP}, with the associated data dictionary in table~\ref{table:OLTPDataDict}. The core of the database is the \code{transactions} table, a registry of all financial transactions occurring within Stripe scope. Tables containing information about merchants and customers are cached from the reference database. Fraud indicators are stored in a dedicated table in a one-to-one correspondence with the main transactions table. The rationale behind this choice is that fraud indicators originate from a different pipeline (e.g. reports after bank investigation).


\begin{figure}[!htb]
	\centering
	\includegraphics[scale=0.6]{./figures/OLTP}
	\caption{Proposed OLTP database structure.}
	\label{fig:OLTP}
\end{figure}


\begin{table}[!htb]
	\centering
	\begin{threeparttable}
		\caption{Data dictionary for the \code{transactions} OLTP schema. The reference tables (e.g. \code{customers}) are described in table~\ref{table:ReferenceDataDict}.}
		\label{table:OLTPDataDict}
		\begin{tabularx}{0.99\textwidth}{l c >{\arraybackslash}X >{\arraybackslash}X}
			\toprule
			\multicolumn{1}{c}{\textbf{Field Name}} & \multicolumn{1}{c}{\textbf{Type}} & \multicolumn{1}{c}{\textbf{Description}} & \multicolumn{1}{c}{\textbf{Example}} \\
			\midrule
			\multicolumn{4}{c}{\code{transactions} table}\\
			\code{transaction\_id} & \code{bigint} & Unique transaction id & \code{123456789} \\
			\code{merchant\_id} & \code{bigint} & Merchant id & \code{12345} \\
			\code{customer\_id} & \code{bigint} & Customer id & \code{234567} \\
			\code{timestamp} & \code{datetime} & UTC transaction timestamp & \code{2023-11-18T17:43:02.4} \\
			\code{amount} & \code{float} & Transaction amount & \code{43.15} \\
			\code{fee} & \code{float} & Transaction fee & \code{0.53} \\
			\code{currency\_code} & \code{char(3)} & Currency code (ISO 4217) & \code{'GBP'} \\
			\code{payment\_method} & \code{text} & Payment method & \code{'credit\_card'} \\
			\code{payment\_status} & \code{text} & Transaction status & \code{'sucess'} \\
			\code{device\_type} & \code{text} & Device used for payment & \code{'mobile'} \\
			\code{ip\_latitude} & \code{float} & IP-based geolocation latitude & \code{49.6833300} \\
			\code{ip\_longitude} & \code{float} & IP-based geolocation longitude & \code{10.5333300} \\
			
			\midrule
			\multicolumn{4}{c}{\code{fraud\_indicators} table}\\
			\code{transaction\_id} & \code{bigint} & Transaction id & \code{123456789} \\
			\code{is\_fraud} & \code{boolean} & Is the transaction a fraud ? & \code{false} \\
			\bottomrule
		\end{tabularx}
	\end{threeparttable}
\end{table}


To support ACID properties and minimize latency, the system uses row-level locking and optimized indexing (e.g., on transaction date or status). Partitioning strategies (e.g., by region or time) are considered for scalability, and real-time replication mechanisms are in place to ensure high availability and failover support.

This OLTP model serves as the authoritative source of truth and is tightly integrated with analytical and NoSQL systems via change data capture (CDC) pipelines, ensuring that derived systems always reflect the most current transactional state.


\clearpage
\section*{Online Analytical Processing (OLAP) Data Model}

The OLAP data model is designed to support complex analytical queries and business intelligence use cases at Stripe, such as revenue tracking, fraud trend analysis, customer segmentation, and regulatory reporting. The model follows a star schema architecture, centered around well-defined fact tables and surrounded by dimension tables to enable fast aggregations and multidimensional analysis, as shown in figure~\ref{fig:OLAP}.

The primary fact table, \code{transactions}, records individual transactional data including metrics such as total amount, fee (Stripe revenue), refund counts, location, fraud status, etc. It links to several dimension tables such as \code{customers}, \code{merchants}, \code{datetimes}, \code{ip\_geography}, \code{payment\_methods}, etc. These dimensions provide the context needed for slicing and dicing the data across various business axes like time, location, customer attributes, or payment types. To optimize performance, pre-aggregated summary tables (e.g., daily or weekly rollups) and materialized views are implemented for high-frequency queries, such as monthly revenue per merchant or fraud rates per region (not represented on figure~\ref{fig:OLAP}).

The OLAP model is refreshed from the OLTP system using Change Data Capture (CDC) and transformed to fit the analytical schema. Role-based access controls and data anonymization are applied to ensure sensitive data remains secure and compliant with privacy regulations.


\begin{figure}[!htb]
	\centering
	\includegraphics[scale=0.58]{./figures/OLAP}
	\caption{Proposed OLAP database structure. The star-schema is centered around the \code{transactions} fact table. Additional pre-aggregated tables (e.g. monthly revenues) are not shown.}
	\label{fig:OLAP}
\end{figure}


\begin{table}[!htb]
	\centering
	\begin{threeparttable}
		\caption{Data dictionary for the main tables in \code{transactions} OLAP schema. The reference tables (e.g. \code{customers}) are described in table~\ref{table:ReferenceDataDict}.}
		\label{table:OLAPDataDict}
		\begin{tabularx}{0.99\textwidth}{l c >{\arraybackslash}X >{\arraybackslash}X}
			\toprule
			\multicolumn{1}{c}{\textbf{Field Name}} & \multicolumn{1}{c}{\textbf{Type}} & \multicolumn{1}{c}{\textbf{Description}} & \multicolumn{1}{c}{\textbf{Example}} \\
			\midrule
			\multicolumn{4}{c}{\code{transactions} table}\\
			\code{transaction\_id} & \code{bigint} & Unique transaction id & \code{123456789} \\
			\code{merchant\_id} & \code{bigint} & Merchant id & \code{12345} \\
			\code{customer\_id} & \code{bigint} & Customer id & \code{234567} \\
			\code{timestamp} & \code{datetime} & UTC transaction timestamp & \code{2023-11-18T17:43:02.4} \\
			\code{amount} & \code{float} & Transaction amount & \code{43.15} \\
			\code{fee} & \code{float} & Transaction fee & \code{0.53} \\
			\code{currency\_code} & \code{char(3)} & Currency code (ISO 4217) & \code{'GBP'} \\
			\code{payment\_method\_id} & \code{integer} & Payment method id & \code{1} \\
			\code{payment\_status\_id} & \code{integer} & Payment status id & \code{2} \\
			\code{device\_type\_id} & \code{integer} & Device id & \code{3} \\
			\code{ip\_geo\_id} & \code{bigint} & IP geolocation id & \code{123456} \\
			\code{is\_fraud} & \code{boolean} & Is the transaction a fraud ? & \code{false} \\
			\code{fraud\_score} & \code{float} & Predicted fraud probability & \code{0.12} \\
			
			\midrule
			\multicolumn{4}{c}{\code{datetimes} table}\\
			\code{timestamp} & \code{datetime} & UTC transaction timestamp & \code{2023-11-18T17:43:02.4} \\
			\code{timezone} & \code{integer} & Timezone offset in minutes & \code{-120} for UTC-02:00 \\
			\code{date} & \code{date} & Transaction date & \code{2023-11-18} \\
			\code{time} & \code{time} & UTC transaction time & \code{17:43:02.4} \\
			\code{year} & \code{mediumint} & Transaction year & \code{2023} \\
			\code{month} & \code{tinyint} & Transaction month & \code{11} \\
			\code{week\_day} & \code{tinyint} & Transaction week day & \code{6} (saturday) \\
			\code{quarter} & \code{tinyint} & Transaction quarter & \code{4} \\
			
			\midrule
			\multicolumn{4}{c}{\code{ip\_geography} table}\\
			\code{ip\_geo\_id} & \code{bigint} & IP geolocation id & \code{123456} \\
			\code{latitude} & \code{float} & IP-based geolocation lat. & \code{49.6833300} \\
			\code{longitude} & \code{float} & IP-based geolocation long. & \code{10.5333300} \\
			\code{country\_code} & \code{char(2)} & ISO 3166-1 alpha-2 & \code{'DE'} \\
			\code{province} & \code{text} & Province name & \code{'Darmstadt'} \\
			
			\bottomrule
		\end{tabularx}
	\end{threeparttable}
\end{table}


\clearpage
\section*{NoSQL Data Model}

Stripe must handle large volumes semi-structured and unstructured data that do not fit well into traditional relational models:
\begin{itemize}[itemsep=5pt, parsep=0pt]
	\item \textbf{Telemetry}. Click stream events, user interactions, session data from the payment API.
	\item \textbf{Customer feedback}. Surveys responses, reviews, support transcripts.
	\item \textbf{Log data}. Access and error logs, API usage.
	\item \textbf{Machine learning features}. Generated or derived features used in model inference.
\end{itemize}

The recommended model follows a document-based schema, implemented with tools such as MongoDB. This choice allows for flexibility in handling nested and heterogeneous data structures. Some key collections of the database are presented in figure~\ref{fig:NoSQL}.

\begin{figure}[!htb]
	\centering
	\includegraphics[scale=0.66]{./figures/NoSQL}
	\caption{Some key collections of the NoSQL system: \code{logs}, \code{session\_data} and \code{customer\_feedback}. Schema made with \href{https://hackolade.com/}{Hackolade} .}
	\label{fig:NoSQL}
\end{figure}


Relationships between documents are managed using a mix of embedding (for tightly coupled data like session actions) and referencing (for scalable lookups, e.g., \code{customer\_id} to feedback history). Indexing strategies are applied on common query fields like timestamps, customer IDs, and event types to optimize performance.

The NoSQL database supports real-time analytics by integrating with data streaming tools (e.g., Apache Kafka or MongoDB Change Streams) and serves as a critical component in powering ML pipelines, recommendation engines, and monitoring dashboards. Sensitive data is either excluded or anonymized, and access is restricted via roles to ensure compliance and reduce exposure risk.


\clearpage
\section*{Security and Compliance}

Stripe handles highly sensitive user data, including banking and payment information. Securing this data is critical for two main reasons: first, to comply with international and regional regulations such as GDPR and PCI-DSS; and second, to preserve customer trust. A data breach could result in reputational damage and potential revenue loss.

To minimize the attack surface, sensitive data is isolated within the OLTP system, where it is encrypted both at rest and in transit. Along with the reference database, this system serves as the primary and most secure repository for confidential information.

Some of this data must be made available for analytical purposes, such as detecting financial crimes (e.g., fraud or money laundering). To mitigate risks, the OLAP system only stores anonymized or tokenized derivatives of sensitive fields, such as bank location or institution name, with no direct identifiers. These fields are not encrypted to preserve query performance, but access is strictly controlled through role-based permissions and secure data transfer protocols.

The NoSQL system does not store any sensitive information. Instead, it is used for storing and indexing semi-structured and unstructured data, such as logs or behavioral metadata, none of which poses regulatory risks if properly managed.

Files are handled according to their security classification. Highly sensitive files, like bank receipts, are stored in a dedicated, access-restricted data lake and are indexed only through the OLTP system (not represented in the main architecture diagram). All other files are similarly stored in a general-purpose data lake and indexed via the NoSQL system.

Encrypted backups are created and refreshed at regular intervals to ensure disaster recovery capabilities in the event of a major incident.

Finally, a centralized logging system tracks all server connections, data accesses, and queries. Anomaly detection mechanisms can be integrated to identify and alert on suspicious activities in real time.


\section*{Machine Learning Integration}

Machine learning (ML) plays a critical role in Stripeâ€™s data ecosystem, enabling real-time fraud detection, customer segmentation, and predictive analytics. These use cases require a tightly integrated ML pipeline that supports both batch and real-time processing, and can handle structured as well as unstructured data. The pipeline fulfills three primary functions:
\begin{itemize}
	\item \textbf{Feature extraction and model training}. Features are extracted from OLAP and NoSQL systems, with orchestration handled by tools such as Apache Airflow. Models are trained in cloud-based environments (e.g., AWS EC2) and managed using a model registry (e.g., MLflow) to ensure traceability and version control.
	\item \textbf{Real time inference}. Models are exposed via APIs and queried asynchronously, typically by the OLAP system, to support real-time decision-making (e.g., fraud scoring). Inference responses can trigger automated actions such as alerts or transaction blocks. Models are deployed within a high-availability infrastructure (e.g., Kubernetes) to ensure scalability and low-latency responses under high transaction loads.
	\item \textbf{Batch predictions}. Models are also executed on a scheduled basis to generate insights for use cases like churn prediction or personalization. The resulting predictions are written back to OLAP or NoSQL systems, depending on their usage, and are consumed by recommendation engines or business intelligence tools.
\end{itemize}

Monitoring tools are deployed to track data drift, feature distribution changes, and prediction accuracy over time. Alerts are triggered when drift exceeds defined thresholds. Retraining can be initiated either on a fixed schedule or manually by an operator based on monitoring insights.


\end{document}