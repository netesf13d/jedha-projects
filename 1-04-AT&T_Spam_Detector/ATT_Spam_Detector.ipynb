{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bad6061b-8baa-4ab4-add0-db674cf11620",
   "metadata": {},
   "source": [
    "# **AT&T Spam Detector**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Contents\n",
    "--------\n",
    "1. [Data loading and preprocessing](#loading)\n",
    "2. [Preliminary data analysis](#eda)\n",
    "3. [Benchmarking with a logistic regression](#logreg)\n",
    "4. [Text processing](#text_processing)\n",
    "4. [Conclusion and perspectives](#conclusion)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4330265c",
   "metadata": {},
   "source": [
    "## <a id=\"loading\"></a> Data loading and preprocessing\n",
    "\n",
    "The dataset consists of 5574 entries, many of which have issues. Here is a non-exhaustive list:\n",
    "- The file doesn't decode as `UTF-8`. It must be decoded as `latin-1` or `mac-roman`.\n",
    "- Some rows are ill-formed (eg line 101 of the csv: `ham\"\"\",,,`)\n",
    "- Most rows have trailing commas\n",
    "- Some messages are anclosed in (possibly many) double quote characters\n",
    "- Single and double quotes are escaped with backslashes\n",
    "- Some spurious characters are present (eg `å` characters that systematically prepend pound symbols `£`)\n",
    "\n",
    "Fortunately, the original version of the dataset can be found [here](https://archive.ics.uci.edu/dataset/228/sms+spam+collection). Its entries are much cleaner than those of our dataset. We can use them as a comparison to setup the cleaning and processing for our dataset:\n",
    "- Strip the trailing commas\n",
    "- Completely remove the double quotes `\"`\n",
    "- Convert backslashes + single quote `\\'` into single quote `'`\n",
    "- Convert backslashes `\\` into double quotes `\"`\n",
    "- Remove the spurious characters `å`\n",
    "\n",
    "The implementation of this pipeline is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6587acf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Loading and preprocessing the project dataset ##########\n",
    "with open('./spam.csv', 'rt', encoding='latin-1') as f:\n",
    "    f.readline() # remove header\n",
    "    raw_data = [row.strip().split(',', 1) for row in f.readlines()]\n",
    "\n",
    "## set target\n",
    "is_spam = np.array([True if row[0] == 'spam' else False for row in raw_data])\n",
    "\n",
    "## process and clean messages\n",
    "messages = [row[1].strip(',') \\\n",
    "                  .replace('\"', '') \\\n",
    "                  .replace(\"\\\\'\", \"'\") \\\n",
    "                  .replace('\\\\', '\"') \\\n",
    "                  .replace('å', '')\n",
    "            for row in raw_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed1f96f",
   "metadata": {},
   "source": [
    "However, some encoding issues persist with some problematic characters still present in messages. To proceed with the project we will rather use this original dataset as the starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c04177",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Loading and preprocessing the original dataset ##########\n",
    "with open('./SMSSpamCollection', 'rt', encoding='utf-8') as f:\n",
    "    raw_data = [row.strip().split('\\t', 1) for row in f.readlines()]\n",
    "\n",
    "is_spam = np.array([True if row[0] == 'spam' else False for row in raw_data])\n",
    "messages = np.array([row[1] for row in raw_data], dtype=object)\n",
    "\n",
    "df = pd.DataFrame({'message': messages, 'is_spam': is_spam})\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb955c59",
   "metadata": {},
   "source": [
    "Our dataset seems to have duplicates. We choose not to remove them, they certainly represent frequent messages for which making a classification error should be penalized more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac8e2e9",
   "metadata": {},
   "source": [
    "## <a id=\"eda\"></a> Preliminary data analysis\n",
    "\n",
    "The messages are quite complex, mixing  uppercase, lowercase, puctuation, alphanumeric and other exotic characters. Training a model to the data would certainly require some form of text normalization. However, this might cause a loss of information.\n",
    "\n",
    "To get some insights about what makes a message a spam and make visualizations, we will reduce the messages to a few descriptive quantities:\n",
    "- `msg_len`, the length of the message;\n",
    "- `chr_caps`, the number of capitallized characters;\n",
    "- `chr_digit`, the number of digit characters in the message;\n",
    "- `chr_punct`, the number of punctuation characters in the message;\n",
    "- `max_wd_len`, he length of the longest character string (split by whitespaces);\n",
    "- `lex_money`, the number of some money-related words and characters: `'$'`, `'£'`, `'€'`, `'cash'`, `'free'`, `'price'` and `'prize'`. The presence of these tokens indicates that the message talks about money (after all, spamming is about getting money from people). We could also consider adding words related to the lexical field of sex.\n",
    "- `caps_first`, the number of consecutive capitallized characters at the beginning of the message.\n",
    "\n",
    "For the purpose of data visualization, we remove the duplicates in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cebbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(msgs: pd.DataFrame, tokens: list['str'])-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Count the number of tokens in the token list from the casefolded messages.\n",
    "    \"\"\"\n",
    "    regex = '|'.join(tokens)\n",
    "    return msgs.apply(lambda x: len(re.findall(regex, x.casefold())))\n",
    "    \n",
    "\n",
    "def count_first_caps(s: str)-> int:\n",
    "    \"\"\"\n",
    "    Count the number of capitallized letters at the beginning of a string.\n",
    "    \"\"\"\n",
    "    n = 0\n",
    "    for c in s:\n",
    "        if not c.isupper():\n",
    "            return n\n",
    "        n += 1\n",
    "    return n\n",
    "\n",
    "\n",
    "def build_features(msgs: pd.DataFrame)-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract quantitative features from a series of messages.\n",
    "    \"\"\"\n",
    "    data = {\n",
    "        'msg_len': msgs.apply(len),\n",
    "        'chr_caps': msgs.apply(lambda s: sum(c.isupper() for c in s)),\n",
    "        'chr_digit': msgs.apply(lambda s: sum(c.isdigit() for c in s)),\n",
    "        'chr_punct': msgs.apply(lambda s: sum(c in punctuation for c in s)),\n",
    "        'max_wd_len': msgs.apply(lambda s: max(len(x) for x in s.split(' '))),\n",
    "        'lex_money': count_tokens(msgs, [r'\\$', '£', '€', 'free', 'cash', 'price', 'prize']),\n",
    "        'caps_first': msgs.apply(count_first_caps)\n",
    "        }\n",
    "    return pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15563bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare dataset from extracted features\n",
    "df_ = df.drop_duplicates()\n",
    "X = build_features(df_['message'])\n",
    "y = df_['is_spam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2be0fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_spam = X.loc[y]\n",
    "X_spam.describe().applymap(lambda x: f\"{x:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0623b3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ham = X.loc[~y]\n",
    "X_ham.describe().applymap(lambda x: f\"{x:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c978ad",
   "metadata": {},
   "source": [
    "Some of our derived features are actually quite discriminating for the spam/ham nature of the messages.\n",
    "- The number of digits is clearly the most discriminating feature with a mean of 15 for spams vs 0.3 for hams. 75 % of spams have more than 10 digits while less than 25% have a single digit.\n",
    "- The presence of money-related words 'free', 'cash' and currency characters is also discriminative of spams, although many spams do not have any.\n",
    "- Spams tend to also have larger message lengths, capitallized characters and punctuation characters. However, there is more overlap between the two categories in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599a91f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1, v2 = 'chr_digit', 'max_wd_len'\n",
    "\n",
    "## number of hams with given values of `v1` and `v2`\n",
    "count_ham = X_ham.loc[:, [v1, v2]].value_counts()\n",
    "x_ham, y_ham = count_ham.index.to_frame().to_numpy().T\n",
    "\n",
    "## number of spams with given values of `v1` and `v2`\n",
    "count_spam = X_spam.loc[:, [v1, v2]].value_counts()\n",
    "x_spam, y_spam = count_spam.index.to_frame().to_numpy().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9706e445",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, ax1 = plt.subplots(\n",
    "    nrows=1, ncols=1, figsize=(5.5, 4.8), dpi=200,\n",
    "    gridspec_kw={'left': 0.12, 'right': 0.92, 'top': 0.9, 'bottom': 0.10}\n",
    ")\n",
    "fig1.suptitle('Figure 1: Scatter plot of messages characteristics',\n",
    "              x=0.02, ha='left')\n",
    "\n",
    "sc_ham = ax1.scatter(x_ham, y_ham, count_ham+2,\n",
    "                     c='blue', alpha=0.65, marker='o', linewidths=0,\n",
    "                     label='hams')\n",
    "sc_spam = ax1.scatter(x_spam, y_spam, count_spam+2,\n",
    "                      c='red', alpha=0.7, marker='o', linewidths=0,\n",
    "                      label='spams')\n",
    "\n",
    "ax1.set_xlim(-2, 50)\n",
    "ax1.set_xticks(np.arange(5, 50, 5), minor=True)\n",
    "ax1.set_xlabel('Number of digits `chr_digits`')\n",
    "\n",
    "ax1.set_ylim(-0.5, 60)\n",
    "ax1.set_ylabel('Max word length `max_wd_len`', labelpad=7)\n",
    "\n",
    "ax1.grid(visible=True, which='major', linewidth=0.2)\n",
    "ax1.grid(visible=True, which='minor', linewidth=0.1)\n",
    "\n",
    "legend = ax1.legend()\n",
    "legend.legend_handles[0]._sizes = [30]\n",
    "legend.legend_handles[1]._sizes = [30]\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033b8c72",
   "metadata": {},
   "source": [
    "We show on figure 1 a scatter plot of the number of digits and max word length for both hams (blue) and spams (red). The surface of the points is proportional to the number of messages (with an offset of 2 to facilitate vizualization). Most hams have few digits and are made of short words. We expect that a simple linear model in our engineered feature space will perform very well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f1f029",
   "metadata": {},
   "source": [
    "## <a id=\"logreg\"></a> Benchmarking with a logistic regression\n",
    "\n",
    "We set up a simple logistic regression here as a benchmark for comparison with more advanced deep learning methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a1a689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c42e6ac2",
   "metadata": {},
   "source": [
    "ChatGPT prompt:\n",
    "\n",
    "I need yout to "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55458140-1bbf-4ff6-a3ce-d74c4a3dad1c",
   "metadata": {},
   "source": [
    "## <a id=\"perpectives\"></a>Perspectives\n",
    "\n",
    "Possible extensions with more data/more precise data.\n",
    "\n",
    "- Zipcode : do people prfer those who live closer\n",
    "- Sex orientation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "root-me",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
