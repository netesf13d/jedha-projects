{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16166ceb",
   "metadata": {},
   "source": [
    "# Data Collection and Storage\n",
    "\n",
    "This notebook implements describes the collection and storage of data used for the project application.\n",
    "\n",
    "Contents\n",
    "--------\n",
    "1. [Fetching data through API calls](#api)\n",
    "    A. [Get geocoding information](#geocoding)\n",
    "    B. [Get weather forecast](#weather)\n",
    "2. [Web scraping](#scraping)\n",
    "    A. [Reverse engineering the website requests](#scraping_reverse)\n",
    "    B. [Get hotel information by scraping](#scraping_fetch)\n",
    "3. [Storage in a database](#database)\n",
    "    A. [Scraping utilities](#scraping_utils)\n",
    "4. [Storage in a data lake](#datalake)\n",
    "    A. [Scraping utilities](#scraping_utils)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76f2e91",
   "metadata": {},
   "source": [
    "## <a name=\"api\"></a>API Calls\n",
    "\n",
    "We request APIs to get geocoding information from the name of a place and weather forecast at given geographic coordinates. Utilities for the corresponding API calls are defined in the module `etl/api_mgmt.py`.\n",
    "\n",
    "To use them, we first setup a `requests.Session`. Since the API servers may limit the number of allowed requests, we add a retry policy to the HTTP session. We also load a file containing the names of the locations of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5df9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter, Retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b12a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup session with retry policy in case of failure\n",
    "s = requests.Session()\n",
    "retries = Retry(total=5, backoff_factor=1, status_forcelist=[403, 502, 503, 504])\n",
    "s.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "\n",
    "# Load locations of interest\n",
    "with open(\"./data/locations.csv\", 'rt', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    next(reader, None) # remove header\n",
    "    locations = [f\"{row[0]}, {row[1]}\" for row in reader]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9549f80",
   "metadata": {},
   "source": [
    "### <a name=\"geocoding\"></a>Get geocoding information\n",
    "\n",
    "We use [Nominatim API](https://nominatim.org/) to fetch geocoding information. The API is quite restrictive in its [use policy](https://operations.osmfoundation.org/policies/nominatim/). Most importantly, the number of requests is limited to one per second, which forces us to throttle the rate accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2862d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from etl import get_coords\n",
    "\n",
    "coordinates = {}\n",
    "for loc in locations:\n",
    "    coordinates[loc] = get_coords(loc)\n",
    "    time.sleep(1.1)\n",
    "\n",
    "coordinates['Rouen, France']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d4a818",
   "metadata": {},
   "source": [
    "### <a name=\"weather\"></a>Get weather forecast\n",
    "\n",
    "We use [Open-Meteo API](https://open-meteo.com/en/docs) to get weather forecast information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f47b8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from etl import get_weather_forecast\n",
    "\n",
    "weather_forecast = {}\n",
    "for loc, coords in coordinates.items():\n",
    "    weather_forecast[loc] = get_weather_forecast(s, **coords)\n",
    "\n",
    "weather_forecast['Rouen, France']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38442342",
   "metadata": {},
   "source": [
    "## <a name=\"scraping\"></a>Web scraping\n",
    "\n",
    "The collection of hotels information at the selected locations is done through web scraping of [booking.com](https://www.booking.com). This approach is more complex and unstable than API calls. It is standard practice to first study how requests and responses are related to web browser interaction. This will allow us to tailor automated requests for scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d311c1",
   "metadata": {},
   "source": [
    "### <a name=\"scraping_reverse\"></a>Reverse engineering the website requests\n",
    "\n",
    "The first step is to get the request resulting from regular user interaction. We thus go to the index page [https://www.booking.com/index.en-gb.html](https://www.booking.com/index.en-gb.html) and fill the search bar. Here we look for an hotal in Rouen, France for 2 adults between march, 1st and march, 9th.\n",
    "\n",
    "<img src=\"media/booking_search.png\" alt=\"booking_search\" width=\"1000\"/>\n",
    "\n",
    "After clicking on \"Search\" the request actually sent by the web browser is displayed in the address bar. This allows us to recover the parameters of the GET request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ab4a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ss=Rouen%2C+France',\n",
       " 'efdco=1',\n",
       " 'label=gen173nr-1FCAEoggI46AdICVgEaE2IAQGYAQm4ARjIAQ_YAQHoAQH4AQKIAgGoAgS4AsWf4r0GwAIB0gIkYmMwNmI1MjktMzkyZS00N2FjLTllNWYtOWZmZGIwMWZjODhj2AIF4AIB',\n",
       " 'aid=304142',\n",
       " 'lang=en-gb',\n",
       " 'sb=1',\n",
       " 'src_elem=sb',\n",
       " 'src=index',\n",
       " 'dest_id=-1462807',\n",
       " 'dest_type=city',\n",
       " 'ac_position=0',\n",
       " 'ac_click_type=b',\n",
       " 'ac_langcode=en',\n",
       " 'ac_suggestion_list_length=5',\n",
       " 'search_selected=true',\n",
       " 'search_pageview_id=a6f166e2b250077a',\n",
       " 'ac_meta=GhBhNmYxNjZlMmIyNTAwNzdhIAAoATICZW46DVJvdWVuLCBGcmFuY2VAAEoAUAA%3D',\n",
       " 'checkin=2025-03-01',\n",
       " 'checkout=2025-03-09',\n",
       " 'group_adults=2',\n",
       " 'no_rooms=1',\n",
       " 'group_children=0']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "req_url = 'https://www.booking.com/searchresults.en-gb.html?ss=Rouen%2C+France&efdco=1&label=gen173nr-1FCAEoggI46AdICVgEaE2IAQGYAQm4ARjIAQ_YAQHoAQH4AQKIAgGoAgS4AsWf4r0GwAIB0gIkYmMwNmI1MjktMzkyZS00N2FjLTllNWYtOWZmZGIwMWZjODhj2AIF4AIB&aid=304142&lang=en-gb&sb=1&src_elem=sb&src=index&dest_id=-1462807&dest_type=city&ac_position=0&ac_click_type=b&ac_langcode=en&ac_suggestion_list_length=5&search_selected=true&search_pageview_id=a6f166e2b250077a&ac_meta=GhBhNmYxNjZlMmIyNTAwNzdhIAAoATICZW46DVJvdWVuLCBGcmFuY2VAAEoAUAA%3D&checkin=2025-03-01&checkout=2025-03-09&group_adults=2&no_rooms=1&group_children=0'\n",
    "\n",
    "req_url.split('?')[1].split('&')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c216cd85",
   "metadata": {},
   "source": [
    "We can already get a few insights from the request URL:\n",
    "- `ss` (search string) corresponds to the text written in the search bar,\n",
    "- `checkin` and `checkout` correspond to the travel dates (the calendar widget in the center),\n",
    "- `group_adults`, `no_rooms` and `group_children` correspond to the input of the right widget.\n",
    "\n",
    "It turns out that a valid request can be made with a different approach, by specifying only the latitude and longitude of the destination. For instance, the URL\n",
    "`'https://www.booking.com/searchresults.en-gb.html?latitude=49.4404591&longitude=1.0939658'` yields a page with the hotels ranked by inreasing distance to the coordinates. The parameters above can also be specified to refine the search, but we will not use them here. The picture below shows the results obtaines after requesting for the above URL. Note how the hotels are sorted by increasing distance from the coordinates.\n",
    "\n",
    "<img src=\"media/booking_optimized_request.png\" alt=\"booking_opt_search\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca07a710",
   "metadata": {},
   "source": [
    "### <a name=\"scraping_fetch\"></a>Get hotel information by scraping\n",
    "\n",
    "The above analysis helped to setup the scraping functionality, which is implemented in the module `etl/scraping_mgmt.py`. Let us detail briefly the sraping procedure:\n",
    "- We reach the target website through an automated browser driven with Selenium WebDriver. We favor this approach over other possibilities such as using `scrapy`. The reason is that our target, booking.com, implements infinite scrolling. This feature is implemented in javascript and is complex to trigger if the scraping tool used cannot execute javascript. A browser, however, natively executes javascript and therefore suits better our task.\n",
    "- For each location, we send a request with the URL `'https://www.booking.com/searchresults.en-gb.html?latitude={latitude}&longitude={longitude}'`.\n",
    "- We scrape the hotels data that we need, scrolling down if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ace075",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de55f594",
   "metadata": {},
   "source": [
    "## <a name=\"database\"></a>Storage in a database\n",
    "\n",
    "For long-term storage, the collected data is transferred a data lake in csv format. We use an AWS S3 bucket for that purpose. The functionality to transfer and load data from the database is implemented in the module `etl/db_mgmt.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd875bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae20ce7d",
   "metadata": {},
   "source": [
    "## <a name=\"datalake\"></a>Storage in a data lake\n",
    "\n",
    "For long-term storage, the collected data is transferred a data lake in csv format. We use an AWS S3 bucket for that purpose. The functionality to transfer and load data from the data lake is implemented in the module `etl/s3_mgmt.py`. We format our data in multiple csv file matching the database structure before uploading in the S3 bucket. A copy of the files is then downloaded and stored locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b6b7b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882e1d78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
