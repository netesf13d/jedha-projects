{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16166ceb",
   "metadata": {},
   "source": [
    "# Data Collection and Storage\n",
    "\n",
    "This notebook implements describes the collection and storage of data used for the project application.\n",
    "\n",
    "Contents\n",
    "--------\n",
    "1. [Fetching data through API calls](#api)\n",
    "    1. [Get geocoding information](#geocoding)\n",
    "    2. [Get weather forecast](#weather)\n",
    "2. [Web scraping](#scraping)\n",
    "    1. [Reverse engineering the website requests](#scraping_reverse)\n",
    "    2. [Get hotel information by scraping](#scraping_fetch)\n",
    "3. [Storage in a database](#database)\n",
    "4. [Storage in a data lake](#datalake)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76f2e91",
   "metadata": {},
   "source": [
    "## <a name=\"api\"></a>API Calls\n",
    "\n",
    "We request APIs to get geocoding information from the name of a place and weather forecast at given geographic coordinates. Utilities for the corresponding API calls are defined in the module `etl/api_mgmt.py`.\n",
    "\n",
    "To use them, we first setup a `requests.Session`. Since the API servers may limit the number of allowed requests, we add a retry policy to the HTTP session. We also load a file containing the names of the locations of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5df9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "\n",
    "from etl import get_coords, get_weather_forecast, save_to_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99b12a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup session with retry policy in case of failure\n",
    "s = requests.Session()\n",
    "retries = Retry(total=5, backoff_factor=1, status_forcelist=[403, 502, 503, 504])\n",
    "s.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "\n",
    "# Load locations of interest\n",
    "with open(\"./data/places.csv\", 'rt', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    next(reader, None) # remove header\n",
    "    locations = [row for row in reader]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9549f80",
   "metadata": {},
   "source": [
    "### <a name=\"geocoding\"></a>Get geocoding information\n",
    "\n",
    "We use [Nominatim API](https://nominatim.org/) to fetch geocoding information. The API is quite restrictive in its [use policy](https://operations.osmfoundation.org/policies/nominatim/). Most importantly, the number of requests is limited to one per second, which forces us to throttle the rate accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2862d28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'place': 'Le Havre',\n",
       " 'country': 'France',\n",
       " 'latitude': 49.4938975,\n",
       " 'longitude': 0.1079732}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coordinates = {}\n",
    "for i, loc in enumerate(locations, start=1):\n",
    "    coordinates[i] = ({'place': loc[0], 'country': loc[1]} \n",
    "                      | get_coords(s, f\"{loc[0]}, {loc[1]}\"))\n",
    "    time.sleep(1.1)\n",
    "\n",
    "coordinates[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82ad7abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "locations_cols = ['location_id', 'place', 'country', 'latitude', 'longitude']\n",
    "with open('./data/locations.csv', 'wt', encoding='utf-8', newline='') as f:\n",
    "    writer = csv.writer(f, delimiter=';')\n",
    "    writer.writerow(locations_cols)\n",
    "    for i, coords in coordinates.items():\n",
    "        writer.writerow([i] + [coords[col] for col in locations_cols[1:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d4a818",
   "metadata": {},
   "source": [
    "### <a name=\"weather\"></a>Get weather forecast\n",
    "\n",
    "We use [Open-Meteo API](https://open-meteo.com/en/docs) to get weather forecast information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f47b8a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'latitude': 49.5,\n",
       " 'longitude': 0.099999905,\n",
       " 'generationtime_ms': 0.8412599563598633,\n",
       " 'utc_offset_seconds': 3600,\n",
       " 'timezone': 'Europe/Paris',\n",
       " 'timezone_abbreviation': 'GMT+1',\n",
       " 'elevation': 9.0,\n",
       " 'daily_units': {'time': 'iso8601',\n",
       "  'temperature_2m_max': '°C',\n",
       "  'temperature_2m_min': '°C',\n",
       "  'sunshine_duration': 's',\n",
       "  'precipitation_sum': 'mm'},\n",
       " 'daily': {'time': ['2025-02-23',\n",
       "   '2025-02-24',\n",
       "   '2025-02-25',\n",
       "   '2025-02-26',\n",
       "   '2025-02-27',\n",
       "   '2025-02-28',\n",
       "   '2025-03-01',\n",
       "   '2025-03-02',\n",
       "   '2025-03-03',\n",
       "   '2025-03-04',\n",
       "   '2025-03-05',\n",
       "   '2025-03-06',\n",
       "   '2025-03-07',\n",
       "   '2025-03-08',\n",
       "   '2025-03-09',\n",
       "   '2025-03-10'],\n",
       "  'temperature_2m_max': [11.5,\n",
       "   11.7,\n",
       "   9.6,\n",
       "   9.8,\n",
       "   8.4,\n",
       "   9.2,\n",
       "   9.9,\n",
       "   10.1,\n",
       "   9.8,\n",
       "   10.1,\n",
       "   11.7,\n",
       "   13.9,\n",
       "   12.6,\n",
       "   8.6,\n",
       "   6.7,\n",
       "   7.2],\n",
       "  'temperature_2m_min': [6.5,\n",
       "   9.2,\n",
       "   7.6,\n",
       "   6.0,\n",
       "   6.2,\n",
       "   0.1,\n",
       "   2.9,\n",
       "   2.4,\n",
       "   3.9,\n",
       "   4.2,\n",
       "   6.7,\n",
       "   7.5,\n",
       "   3.9,\n",
       "   6.0,\n",
       "   5.0,\n",
       "   3.1],\n",
       "  'sunshine_duration': [15187.21,\n",
       "   355.16,\n",
       "   28800.0,\n",
       "   14687.29,\n",
       "   35387.39,\n",
       "   30196.69,\n",
       "   36186.56,\n",
       "   36404.88,\n",
       "   36766.64,\n",
       "   36859.07,\n",
       "   36337.85,\n",
       "   37108.68,\n",
       "   37233.57,\n",
       "   9489.57,\n",
       "   37457.49,\n",
       "   37860.5],\n",
       "  'precipitation_sum': [0.0,\n",
       "   2.0,\n",
       "   2.5,\n",
       "   10.6,\n",
       "   0.0,\n",
       "   0.3,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1.0,\n",
       "   2.0,\n",
       "   0.6]}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_forecast = {}\n",
    "for i, coords in coordinates.items():\n",
    "    weather_forecast[i] = get_weather_forecast(\n",
    "        s, coords['latitude'], coords['longitude'])\n",
    "\n",
    "weather_forecast[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "361feca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "# take the average weather over the next 7 days\n",
    "weather_cols = [\n",
    "    'location_id', 'date', 'min_temperature_C', 'max_temperature_C',\n",
    "    'sunshine_duration_h', 'precipitation_sum_mm'\n",
    "]\n",
    "with open('./data/weather_indicators.csv', 'wt', encoding='utf-8', newline='') as f:\n",
    "    writer = csv.writer(f, delimiter=';')\n",
    "    writer.writerow(weather_cols)\n",
    "    for i, forecast in weather_forecast.items():\n",
    "        forecast = forecast['daily']\n",
    "        row = [i, forecast['time'][0],\n",
    "               np.mean(forecast['temperature_2m_min'][:8]),\n",
    "               np.mean(forecast['temperature_2m_max'][:8]),\n",
    "               np.mean(forecast['sunshine_duration'][:8])/3600,\n",
    "               np.mean(forecast['precipitation_sum'][:8])]\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38442342",
   "metadata": {},
   "source": [
    "## <a name=\"scraping\"></a>Web scraping\n",
    "\n",
    "The collection of hotels information at the selected locations is done through web scraping of [booking.com](https://www.booking.com). This approach is more complex and unstable than API calls. It is standard practice to first study how requests and responses are related to web browser interaction. This will allow us to tailor automated requests for scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d311c1",
   "metadata": {},
   "source": [
    "### <a name=\"scraping_reverse\"></a>Reverse engineering the website requests\n",
    "\n",
    "The first step is to get the request resulting from regular user interaction. We thus go to the index page [https://www.booking.com/index.en-gb.html](https://www.booking.com/index.en-gb.html) and fill the search bar. Here we look for an hotal in Rouen, France for 2 adults between march, 1st and march, 9th.\n",
    "\n",
    "<img src=\"media/booking_search.png\" alt=\"booking_search\" width=\"1000\"/>\n",
    "\n",
    "After clicking on \"Search\" the request actually sent by the web browser is displayed in the address bar. This allows us to recover the parameters of the GET request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67ab4a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ss=Rouen%2C+France',\n",
       " 'efdco=1',\n",
       " 'label=gen173nr-1FCAEoggI46AdICVgEaE2IAQGYAQm4ARjIAQ_YAQHoAQH4AQKIAgGoAgS4AsWf4r0GwAIB0gIkYmMwNmI1MjktMzkyZS00N2FjLTllNWYtOWZmZGIwMWZjODhj2AIF4AIB',\n",
       " 'aid=304142',\n",
       " 'lang=en-gb',\n",
       " 'sb=1',\n",
       " 'src_elem=sb',\n",
       " 'src=index',\n",
       " 'dest_id=-1462807',\n",
       " 'dest_type=city',\n",
       " 'ac_position=0',\n",
       " 'ac_click_type=b',\n",
       " 'ac_langcode=en',\n",
       " 'ac_suggestion_list_length=5',\n",
       " 'search_selected=true',\n",
       " 'search_pageview_id=a6f166e2b250077a',\n",
       " 'ac_meta=GhBhNmYxNjZlMmIyNTAwNzdhIAAoATICZW46DVJvdWVuLCBGcmFuY2VAAEoAUAA%3D',\n",
       " 'checkin=2025-03-01',\n",
       " 'checkout=2025-03-09',\n",
       " 'group_adults=2',\n",
       " 'no_rooms=1',\n",
       " 'group_children=0']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "req_url = 'https://www.booking.com/searchresults.en-gb.html?ss=Rouen%2C+France&efdco=1&label=gen173nr-1FCAEoggI46AdICVgEaE2IAQGYAQm4ARjIAQ_YAQHoAQH4AQKIAgGoAgS4AsWf4r0GwAIB0gIkYmMwNmI1MjktMzkyZS00N2FjLTllNWYtOWZmZGIwMWZjODhj2AIF4AIB&aid=304142&lang=en-gb&sb=1&src_elem=sb&src=index&dest_id=-1462807&dest_type=city&ac_position=0&ac_click_type=b&ac_langcode=en&ac_suggestion_list_length=5&search_selected=true&search_pageview_id=a6f166e2b250077a&ac_meta=GhBhNmYxNjZlMmIyNTAwNzdhIAAoATICZW46DVJvdWVuLCBGcmFuY2VAAEoAUAA%3D&checkin=2025-03-01&checkout=2025-03-09&group_adults=2&no_rooms=1&group_children=0'\n",
    "\n",
    "req_url.split('?')[1].split('&')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c216cd85",
   "metadata": {},
   "source": [
    "We can already get a few insights from the request URL:\n",
    "- `ss` (search string) corresponds to the text written in the search bar,\n",
    "- `checkin` and `checkout` correspond to the travel dates (the calendar widget in the center),\n",
    "- `group_adults`, `no_rooms` and `group_children` correspond to the input of the right widget.\n",
    "\n",
    "It turns out that a valid request can be made with a different approach, by specifying only the latitude and longitude of the destination. For instance, the URL\n",
    "`'https://www.booking.com/searchresults.en-gb.html?latitude=49.4404591&longitude=1.0939658'` yields a page with the hotels ranked by inreasing distance to the coordinates. The parameters above can also be specified to refine the search, but we will not use them here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca07a710",
   "metadata": {},
   "source": [
    "### <a name=\"scraping_fetch\"></a>Get hotel information by scraping\n",
    "\n",
    "The above analysis helped to setup the scraping functionality, which is implemented in the module `etl/scraping_mgmt.py`. Let us detail briefly the sraping procedure:\n",
    "- We reach the target website through an automated browser driven with Selenium WebDriver. We favor this approach over other possibilities such as using `scrapy`. The reason is that our target, booking.com, implements infinite scrolling. This feature is implemented in javascript and is complex to trigger if the scraping tool used cannot execute javascript. A browser, however, natively executes javascript and therefore suits better our task.\n",
    "- For each location, we send a request with the URL `'https://www.booking.com/searchresults.en-gb.html?latitude={latitude}&longitude={longitude}'`.\n",
    "- We scrape the hotels data that we need, scrolling down if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03ace075",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "\n",
    "from etl import scrape_from_searchpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d7a1b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## setup driver with options to prevent detection\n",
    "## See https://stackoverflow.com/questions/53039551/selenium-webdriver-modifying-navigator-webdriver-flag-to-prevent-selenium-detec/53040904#53040904\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "options.add_experimental_option('useAutomationExtension', False)\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "driver.execute_cdp_cmd(\n",
    "    'Network.setUserAgentOverride',\n",
    "    {\"userAgent\": 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.53 Safari/537.36'}\n",
    ")\n",
    "driver.implicitly_wait(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab1d0822",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/locations.csv\", 'rt', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=';')\n",
    "    next(reader, None) # remove header\n",
    "    locations = [row for row in reader]\n",
    "\n",
    "search_urls = {loc[0]: ('https://www.booking.com/searchresults.en-gb.html?'\n",
    "                        f'latitude={loc[3]}&longitude={loc[4]}')\n",
    "               for loc in locations}\n",
    "\n",
    "for i, search_url in search_urls.items():\n",
    "    hotel_infos = {i: scrape_from_searchpage(driver, search_url, limit=30)} # scrape on search\n",
    "    save_to_json(f'./data/temp/{i}.json', hotel_infos)\n",
    "    break\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79bde280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load temporary saved files\n",
    "hotels_list = []\n",
    "root, _, files = next(os.walk('./data/temp/'))\n",
    "for file in files:\n",
    "    with open(root + file, 'rt', encoding='utf-8') as f:\n",
    "        hotels_list.append(json.load(f))\n",
    "\n",
    "# transform and save csv\n",
    "hotels_cols = ['hotel_id', 'location_id', 'url', 'name',\n",
    "               'description', 'rating', 'georating']\n",
    "hotel_id = 1\n",
    "hotels_data = []\n",
    "for hotels in hotels_list:\n",
    "    loc_id, hotels = next(iter(hotels.items()))\n",
    "    for h in hotels:\n",
    "        entry = [hotel_id, loc_id] + [h[col] for col in hotels_cols[2:]]\n",
    "        hotels_data.append(entry)\n",
    "        hotel_id += 1\n",
    "\n",
    "with open('./data/hotels.csv', 'wt', encoding='utf-8', newline='') as f:\n",
    "    writer = csv.writer(f, delimiter=';')\n",
    "    writer.writerow(hotels_cols)\n",
    "    for row in hotels_data:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d2955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove temp directory\n",
    "shutil.rmtree('./data/temp/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ec1e75",
   "metadata": {},
   "source": [
    "## <a name=\"datalake\"></a>Storage in a data lake\n",
    "\n",
    "For long-term storage, the collected data is transferred a data lake in csv format. We use an AWS S3 bucket for that purpose. The functionality to transfer and load data from the data lake is implemented in the module `etl/s3_mgmt.py`. We format our data in multiple csv file matching the database structure before uploading in the S3 bucket. A copy of the files is then downloaded and stored locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7820667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b7a63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dde37b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create S3 client\n",
    "region_name = \"eu-west-3\"\n",
    "with open(\"./bucket_name.key\", 'rt', encoding='utf-8') as f:\n",
    "    bucket_name = f.read()\n",
    "with open(\"./jedha-project-s3-writer_accessKeys.key\", 'rt', encoding='utf-8') as f:\n",
    "    aws_access_key_id, aws_secret_access_key = f.readlines()[-1].strip().split(',')\n",
    "\n",
    "s3 = boto3.client('s3', region_name=region_name,\n",
    "                  aws_access_key_id=aws_access_key_id, \n",
    "                  aws_secret_access_key=aws_secret_access_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f73e572",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uplpoad the files created\n",
    "s3.upload_file('./data/locations.csv', Bucket=bucket_name, Key='/data/locations.csv')\n",
    "s3.upload_file('./data/weather_indicators.csv', Bucket=bucket_name, Key='/data/weather_indicators.csv')\n",
    "s3.upload_file('./data/hotels.csv', Bucket=bucket_name, Key='/data/hotels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26763c3d",
   "metadata": {},
   "source": [
    "## <a name=\"database\"></a>Storage in a database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de55f594",
   "metadata": {},
   "source": [
    "## <a name=\"database\"></a>Storage in a database\n",
    "\n",
    "We use a database to load data for the application. The functionality to transfer and load data from the database is implemented in the module `etl/db_mgmt.py`. We use SQLAlchemy to define our database structure, connected to a PostgreSQL database hosted on [Neon](https://neon.tech). The structure of the database is the following.\n",
    "\n",
    "<img src=\"media/DB_structure.png\" alt=\"Database structure\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ac35d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "\n",
    "from sqlalchemy import create_engine, URL, inspect\n",
    "from sqlalchemy import select\n",
    "from sqlalchemy.orm import Session\n",
    "\n",
    "from etl import Base, Location, Hotel, WeatherIndicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd875bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get credentials\n",
    "with open('./neondb_access_keys.key', 'rt', encoding='utf-8') as f:\n",
    "    PGHOST = f.readline().split(\"'\")[1]\n",
    "    PGDATABASE = f.readline().split(\"'\")[1]\n",
    "    PGUSER = f.readline().split(\"'\")[1]\n",
    "    PGPASSWORD = f.readline().split(\"'\")[1]\n",
    "\n",
    "url = URL.create(\n",
    "    \"postgresql+psycopg\",\n",
    "    username=PGUSER,\n",
    "    password=PGPASSWORD,\n",
    "    host=PGHOST,\n",
    "    database=PGDATABASE,\n",
    ")\n",
    "\n",
    "## setup SQL engine\n",
    "engine = create_engine(url, echo=False)\n",
    "# inspector = inspect(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3245dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from S3 buckets\n",
    "with BytesIO() as f:\n",
    "    s3.download_fileobj(bucket_name, '/data/locations.csv', f)\n",
    "    data = f.getvalue().decode('utf-8')\n",
    "\n",
    "with open(\"../data/locations.csv\", 'rt', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=';')\n",
    "    next(reader, None) # remove header\n",
    "    locations = [Location(location_id=row[0], name=row[1], country=row[2],\n",
    "                          latitude=row[3], longitude=row[4])\n",
    "                 for row in reader]\n",
    "\n",
    "weather_indicators = []\n",
    "\n",
    "hotels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345fda06",
   "metadata": {},
   "outputs": [],
   "source": [
    "Base.metadata.create_all(engine)\n",
    "with Session(engine) as session:\n",
    "    session.add_all(locations)\n",
    "    session.add_all(weather_indicators)\n",
    "    session.add_all(hotels)\n",
    "    session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39badd66",
   "metadata": {},
   "source": [
    "The transferred data can be seen on the server, as shown below.\n",
    "\n",
    "<img src=\"media/DB_storage.png\" alt=\"Database structure\" width=\"1200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11b6b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "## read database table directly into dataframe\n",
    "import pandas as pd\n",
    "\n",
    "pd.read_sql(select(Location.__table__), con=engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae20ce7d",
   "metadata": {},
   "source": [
    "## <a name=\"datalake\"></a>Storage in a data lake\n",
    "\n",
    "For long-term storage, the collected data is transferred a data lake in csv format. We use an AWS S3 bucket for that purpose. The functionality to transfer and load data from the data lake is implemented in the module `etl/s3_mgmt.py`. We format our data in multiple csv file matching the database structure before uploading in the S3 bucket. A copy of the files is then downloaded and stored locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b6b7b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882e1d78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
